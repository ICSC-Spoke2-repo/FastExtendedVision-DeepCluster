{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e0104a-1b83-4a8e-aa8c-60efa2018d97",
   "metadata": {},
   "source": [
    "# Defining the general class with init, loading, training, eval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0d880a2-a506-4a2e-83f5-48341348e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#   MAGIC TRICK FOR HAVING tab, shift+tab COMMANDS!\n",
    "#################################################################################\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fbcbaf-afdc-4db3-a1ee-a7baaaf47ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch v.\t1.10.1+cu102\n",
      "TorchVision v.\t0.11.2+cu102\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# https://discuss.pytorch.org/t/cuda-launch-blocking-in-jupyter-notebook/163029\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "from typing import Union\n",
    "import tqdm\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json \n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except: \n",
    "    %pip install torchsummary\n",
    "    from torchsummary import summary\n",
    "\n",
    "\n",
    "print(f\"PyTorch v.\\t{torch.__version__}\")\n",
    "print(f\"TorchVision v.\\t{torchvision.__version__}\\n\")\n",
    "\n",
    "# in torch/pytorch data and models need to be moved in the specific processing unit\n",
    "# this code snippet allows to set the variable \"device\" according to available resoirce (cpu or cuda gpu)\n",
    "if torch.cuda.is_available():\n",
    "    print('number of devices: ', torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\\n\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.memory_utils import free_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90679058-e3fc-479b-b577-63fe1d98ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset utils\n",
    "from utils.memory_utils import free_memory\n",
    "from utils.custom_dataset import XRFAE1DDataset, AstroSynthDataset\n",
    "from utils.pretreatment import custom_transform, normalize_hist, rebin_xrf, tanh_norm, smooth_1d\n",
    "# Model utils\n",
    "from utils.VAE1D_model import DeepClustering_VAE1D\n",
    "from utils.save_best_model import SaveBestModel\n",
    "# Loss utils\n",
    "from utils.losses import vae_loss_function, silhouette_loss, compute_total_loss\n",
    "# Clustering utils\n",
    "from utils.clustering_utils import silhouette_score, IterativeKMeans, KMeans, kpp_init\n",
    "# VAE utils\n",
    "from utils.beta_scheduler import BetaScheduler\n",
    "# Logs utils\n",
    "from utils.logs_utils import write_line_to_file, store_hyp_dict\n",
    "# Loader uytils\n",
    "from utils.open_model import load_model_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48174e-635c-430e-83c4-b00ff56eb3da",
   "metadata": {},
   "source": [
    "## Define the model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaafd3ff-cadf-4ba6-be4d-d028b9412e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClustering_trainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Verbosity\n",
    "        verbosity: int = 1, \n",
    "        # Dataset\n",
    "        CUSTOM_TRANSFORM: bool = False ,\n",
    "        BASE_DATASET_PATH: str = '/jupyter/notebooks/Article/AstroData/Synthetic/1D/' ,\n",
    "        MAX_TRAIN_SIZE: int  = int(20*10e+4) ,\n",
    "        MAX_VAL_SIZE  : int  = int( 4*10e+4) ,\n",
    "        MAX_TEST_SIZE : int  = int( 1*10e+4) ,\n",
    "        # Dataloader\n",
    "        BATCH_SIZE : int = 256, \n",
    "        NUM_WORKERS: int = 6,\n",
    "        DO_NORMALIZE: bool = True,\n",
    "        # Model kwargs\n",
    "        input_dim: int = 1024,\n",
    "        n_layers : int = 4, \n",
    "        encoding_space_dim: int = 32, \n",
    "        latent_space_dim: int = 6, \n",
    "        use_latent_space_activation : bool = True, \n",
    "        pow_2_decrease: bool = True, \n",
    "        pow_2_increase: bool = True, \n",
    "        final_activation = nn.Sigmoid(),\n",
    "        use_SNN: bool = True,         # If true, Self-Normalising Neural networks are used\n",
    "        # K-Means\n",
    "        min_n_cluster: int = 4,\n",
    "        max_n_cluster: int = 12, \n",
    "        Niter: int = 10, \n",
    "        random_centroid_init: bool = False, \n",
    "        # Optimiser\n",
    "        learning_rate: float = 1e-3, \n",
    "        use_lr_scheduler: bool = True, \n",
    "        # Name\n",
    "        BASE_PATH_TO_STORE: str = './model_data',\n",
    "        base_model_name: str = 'DeepClustering',\n",
    "        # training\n",
    "        epochs  : int = 100,\n",
    "        PATIENCE: int = 10,\n",
    "        # beta-VAE param\n",
    "        BETA_MIN: float = 0.0, # NB if BETA_MIN >= BETA_MAX, beta(t) = BETA_MIN forall t\n",
    "        BETA_MAX: float = 0.0,\n",
    "        beta_t_number_of_epochs: int = 60,\n",
    "        beta_t_starting_epoch  : int = 10, \n",
    "        beta_t_ending_epoch    : int = 30,\n",
    "        # Deep Clustering gamma\n",
    "        γ_init : float = 0.05,\n",
    "        USE_VARYING_GAMMA: bool = False,\n",
    "        gamma_t_number_of_epochs: int = 60,\n",
    "        gamma_t_starting_epoch  : int = 10, \n",
    "        gamma_t_ending_epoch    : int = 30,\n",
    "        # Loss\n",
    "        USE_SUM : bool  = True,\n",
    "        # Device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #\n",
    "        self.BASE_PATH_TO_STORE = BASE_PATH_TO_STORE\n",
    "        self.starting_time_str  = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        self.epochs   = epochs\n",
    "        self.PATIENCE = PATIENCE\n",
    "        self.DO_NORMALIZE = DO_NORMALIZE\n",
    "        # VAE?\n",
    "        self.BETA_MIN = BETA_MIN \n",
    "        self.BETA_MAX = BETA_MAX\n",
    "        # Deep Clustering\n",
    "        self.γ_init = γ_init\n",
    "        self.USE_VARYING_GAMMA = USE_VARYING_GAMMA\n",
    "        # loss\n",
    "        self.USE_SUM = USE_SUM\n",
    "        # model name\n",
    "        self.base_model_name = base_model_name\n",
    "        self.model_name  = f'{base_model_name}AE_{self.starting_time_str}' if self.BETA_MIN == self.BETA_MAX == 0 else f'{base_model_name}VAE_{self.starting_time_str}' \n",
    "        self.model_name += '_use_sum' if self.USE_SUM else '' \n",
    "        # create dir\n",
    "        self.create_log_dir()\n",
    "        # other\n",
    "        self.verbosity = verbosity\n",
    "        self.device    = device\n",
    "        self.input_dim = input_dim\n",
    "        # Apply custom transform or not\n",
    "        self.CUSTOM_TRANSFORM  = CUSTOM_TRANSFORM\n",
    "        self.BASE_DATASET_PATH = BASE_DATASET_PATH\n",
    "        self.MAX_TRAIN_SIZE = MAX_TRAIN_SIZE\n",
    "        self.MAX_VAL_SIZE   = MAX_VAL_SIZE\n",
    "        self.MAX_TEST_SIZE  = MAX_TEST_SIZE\n",
    "        # Dataloader\n",
    "        self.BATCH_SIZE  = BATCH_SIZE\n",
    "        self.NUM_WORKERS = NUM_WORKERS\n",
    "        \n",
    "        # INIT DATASET\n",
    "        self.init_dataset()\n",
    "        # INIT DATALOADE\n",
    "        self.init_dataloader()\n",
    "        \n",
    "        # Model kwargs\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers  = n_layers\n",
    "        self.encoding_space_dim = encoding_space_dim\n",
    "        self.latent_space_dim   = latent_space_dim\n",
    "        self.use_latent_space_activation = use_latent_space_activation\n",
    "        self.pow_2_decrease = pow_2_decrease\n",
    "        self.pow_2_increase = pow_2_increase\n",
    "        self.final_activation = final_activation if self.DO_NORMALIZE else nn.ReLU()\n",
    "        self.min_n_cluster = min_n_cluster\n",
    "        self.max_n_cluster = max_n_cluster\n",
    "        self.Niter = Niter\n",
    "        self.random_centroid_init = random_centroid_init\n",
    "        self.use_SNN = use_SNN\n",
    "        self.model_kwargs = {\n",
    "            \"input_dim\" : self.input_dim, \n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"encoding_space_dim\": self.encoding_space_dim, \n",
    "            \"latent_space_dim\": self.latent_space_dim,\n",
    "            \"use_latent_space_activation\": self.use_latent_space_activation,\n",
    "            \"pow_2_decrease\": self.pow_2_decrease ,\n",
    "            \"pow_2_increase\": self.pow_2_increase ,\n",
    "            \"final_activation\": self.final_activation ,\n",
    "            \"min_n_cluster\": self.min_n_cluster, \n",
    "            \"max_n_cluster\": self.max_n_cluster, \n",
    "            \"Niter\": self.Niter, \n",
    "            \"verbose\": True if self.verbosity > 0 else False, \n",
    "            \"random_centroid_init\": self.random_centroid_init,\n",
    "            \"use_SNN\" : self.use_SNN , \n",
    "        }\n",
    "        # init model\n",
    "        self.init_model()\n",
    "        \n",
    "        # Optimiser\n",
    "        self.learning_rate    = learning_rate \n",
    "        self.use_lr_scheduler = use_lr_scheduler\n",
    "        # init optimiser\n",
    "        self.init_adam_optimizer()\n",
    "          \n",
    "        # Training utils\n",
    "        # Save Model\n",
    "        self.save_best_model = SaveBestModel(model_name=f\"{self.full_path_to_store}/{self.model_name}\") #initialize checkpoint function\n",
    "        # Beta Scheduler\n",
    "        self.beta_t_number_of_epochs = beta_t_number_of_epochs\n",
    "        self.beta_t_starting_epoch   = beta_t_starting_epoch\n",
    "        self.beta_t_ending_epoch = beta_t_ending_epoch\n",
    "        self.beta_t = BetaScheduler(\n",
    "            β_min = self.BETA_MIN, β_max = self.BETA_MAX,\n",
    "            number_of_epochs = self.beta_t_number_of_epochs,\n",
    "            starting_epoch   = self.beta_t_starting_epoch, \n",
    "            ending_epoch     = self.beta_t_ending_epoch\n",
    "        )\n",
    "        self.delay_best_model_store = (self.beta_t_starting_epoch > 0 or self.beta_t_ending_epoch > 0) and self.BETA_MAX > 0\n",
    "        self.Delta_patience = self.beta_t_ending_epoch if self.delay_best_model_store else 0\n",
    "        \n",
    "        if self.verbosity > 0: print(f\"Delay best model store: {self.delay_best_model_store};\\nDelta patience: {self.Delta_patience}\")\n",
    "        \n",
    "        self._vae_args = {\n",
    "            'BETA_MIN' : self.BETA_MIN, \n",
    "            'BETA_MAX' : self.BETA_MAX, \n",
    "            'beta_t_number_of_epochs' : self.beta_t_number_of_epochs ,\n",
    "            'beta_t_starting_epoch'   : self.beta_t_starting_epoch ,\n",
    "            'beta_t_ending_epoch'     : self.beta_t_ending_epoch ,\n",
    "            'delay_best_model_store'  : self.delay_best_model_store ,\n",
    "        }\n",
    "        # Gamma scheduler\n",
    "        self.gamma_t_number_of_epochs = gamma_t_number_of_epochs\n",
    "        self.gamma_t_starting_epoch   = gamma_t_starting_epoch\n",
    "        self.gamma_t_ending_epoch  = gamma_t_ending_epoch\n",
    "        self.gamma_t = BetaScheduler(\n",
    "            β_min = 0.0 if self.USE_VARYING_GAMMA else self.γ_init, # if USE_VARYING_GAMMA = False, gamma_t = γ_init\n",
    "            β_max = self.γ_init,\n",
    "            number_of_epochs = self.gamma_t_number_of_epochs,\n",
    "            starting_epoch = self.gamma_t_starting_epoch, \n",
    "            ending_epoch = self.gamma_t_ending_epoch\n",
    "        )\n",
    "        # History\n",
    "        self.training_loss   = []\n",
    "        self.validation_loss = []\n",
    "\n",
    "        self.mmd_losses = []\n",
    "        self.rec_losses = []\n",
    "        self.s_losses   = []\n",
    "\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # === STORE HYPERPARAMETERS =====\n",
    "        self.hyperparam_kwargs = {\n",
    "            # Model kwargs\n",
    "            **self.model_kwargs,\n",
    "            # Dataset info\n",
    "            'train_size': len(self.dataloader_train.dataset),\n",
    "            'val_size'  : len(self.dataloader_val.dataset),\n",
    "            # Hyperparameters\n",
    "            'epochs'   : self.epochs,\n",
    "            'patience' : self.PATIENCE,\n",
    "            'min_n_clusters': self.min_n_cluster,\n",
    "            'max_n_clusters': self.max_n_cluster,\n",
    "            'ItKmeans_n_iter' : self.Niter,\n",
    "            'γ_init' : self.γ_init,\n",
    "            'γ variable'  : self.USE_VARYING_GAMMA,\n",
    "            'loss_use_sum': self.USE_SUM,\n",
    "            # Other hyperparams\n",
    "            **self._vae_args , \n",
    "        }\n",
    "        del self.hyperparam_kwargs['final_activation']\n",
    "        store_hyp_dict(f'{self.full_path_to_store}/{self.model_name}.json', self.hyperparam_kwargs)\n",
    "        if self.verbosity > 0: print(\"Stored JSON\")\n",
    "        # === STORE LOGS =====\n",
    "        self.LOG_FILE = f'{self.full_path_to_store}/{self.model_name}.txt'\n",
    "\n",
    "        log_text =f\"{self.model}\\n\\nLoss used:\\tAECluster\\n\\nTrain Size: {len(self.dataloader_train.dataset)}\\tValidation size: {len(self.dataloader_val.dataset)}\\tBatch size: {self.BATCH_SIZE}\\nEpochs: {self.epochs}\\tPatience: {self.PATIENCE}\\nLatent space activation: {self.use_latent_space_activation}\\n\\n\"\n",
    "        if self.verbosity > 0: print(log_text)\n",
    "        write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=log_text)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def create_log_dir(self):\n",
    "        \"\"\"\n",
    "        Mathod to create the log dir for the model\n",
    "        \"\"\"\n",
    "        path = f'{self.BASE_PATH_TO_STORE}/{self.model_name}'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        self.full_path_to_store = path\n",
    "    \n",
    "    def custom_transform_realized(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Custom pretreatment function.\n",
    "        \n",
    "        If self.CUSTOM_TRANSFORM == True, it applies custom_transform method; else, the normalize_hist(1 + log(x)) method. \n",
    "        \n",
    "        Args: \n",
    "            x   (torch.Tensor): the tensor to be transformed\n",
    "            \n",
    "        \"\"\"\n",
    "        if self.CUSTOM_TRANSFORM:\n",
    "            if x.shape[-1] == self.input_dim:\n",
    "                _ret = smooth_1d(x)\n",
    "            else:\n",
    "                _ret =  rebin_xrf(x, n_bins=self.input_dim) \n",
    "            return normalize_hist( _ret )  if self.DO_NORMALIZE else _ret\n",
    "        else: \n",
    "            if x.shape[-1] == self.input_dim:\n",
    "                return normalize_hist( torch.log(1 + x) )  if self.DO_NORMALIZE else torch.log(1 + x)\n",
    "            else:\n",
    "                return normalize_hist( torch.log(1 + rebin_xrf(x, n_bins=self.input_dim) ) ) if self.DO_NORMALIZE else  torch.log(1 + rebin_xrf(x, n_bins=self.input_dim) )\n",
    "            \n",
    "    def init_dataset(self):\n",
    "        if self.verbosity>0: print(f\"Preparing dataset.\\nUsing custom transform: {self.CUSTOM_TRANSFORM}\\n\")\n",
    "\n",
    "        if self.verbosity>0: print('Train set:')\n",
    "        self.dataset_train = AstroSynthDataset(\n",
    "            path_to_data  = self.BASE_DATASET_PATH + \"train_datacube.pt\" ,\n",
    "            path_to_label = self.BASE_DATASET_PATH + \"train_labels.pt\" ,\n",
    "            transform   = self.custom_transform_realized,\n",
    "            max_size    = self.MAX_TRAIN_SIZE\n",
    "        )\n",
    "        if self.verbosity>0: print('Test set:')\n",
    "        self.dataset_val = AstroSynthDataset(\n",
    "            path_to_data  = self.BASE_DATASET_PATH + \"test_datacube.pt\" ,\n",
    "            path_to_label = self.BASE_DATASET_PATH + \"test_labels.pt\" ,\n",
    "            transform = self.custom_transform_realized,\n",
    "            max_size  = self.MAX_VAL_SIZE\n",
    "        )\n",
    "            \n",
    "        self.dataset_test = AstroSynthDataset(\n",
    "            path_to_data  = self.BASE_DATASET_PATH + \"val_datacube.pt\" ,\n",
    "            path_to_label = self.BASE_DATASET_PATH + \"val_labels.pt\" ,\n",
    "            transform = self.custom_transform_realized,\n",
    "            max_size  = self.MAX_TEST_SIZE\n",
    "        )\n",
    "        if self.verbosity>0: print(\"\\nDone.\")\n",
    "        \n",
    "        if self.verbosity>1:\n",
    "            print(f\"\"\"\n",
    "            Dataset infos:\n",
    "            Train: {len(self.dataset_train)}\n",
    "            Val  : {len(self.dataset_val)}\n",
    "            Test : {len(self.dataset_test)}\n",
    "            \"\"\")\n",
    "\n",
    "    def init_dataloader(self):\n",
    "        # Dataloader\n",
    "        self.dataloader_train = DataLoader(\n",
    "            self.dataset_train, batch_size=self.BATCH_SIZE, shuffle=True,\n",
    "            drop_last=True, pin_memory=True, num_workers=self.NUM_WORKERS\n",
    "        ) \n",
    "        self.dataloader_val   = DataLoader(\n",
    "            self.dataset_val, batch_size=self.BATCH_SIZE, shuffle=False,\n",
    "            drop_last=False, pin_memory=False, num_workers=self.NUM_WORKERS\n",
    "        ) \n",
    "        # Test\n",
    "        self.dataloader_test  = DataLoader(self.dataset_test,  batch_size=self.BATCH_SIZE, shuffle=False) \n",
    "        \n",
    "    def init_model(self):\n",
    "        self.model = DeepClustering_VAE1D(**self.model_kwargs)\n",
    "        self.model.to(self.device)\n",
    "        if self.verbosity > 0: print(self.model)\n",
    "\n",
    "        if self.verbosity > 1: \n",
    "            print(f\"\\n\\nTorchSummary:\\n\")\n",
    "            print(summary(self.model, input_size=( self.input_dim,  ), batch_size=self.BATCH_SIZE, device='cpu'))\n",
    "            \n",
    "    def load_model(self, for_eval: bool = True, load_from_json: bool = False):\n",
    "        try:\n",
    "            if load_from_json:\n",
    "                base_path = self.full_path_to_store if self.full_path_to_store[:2] != './' else self.full_path_to_store[2:]\n",
    "                _Name_to_open = f\"{base_path}/{self.model_name}\" \n",
    "                self.model = load_model_func(_Name_to_open, _model_kwargs=self.model_kwargs, model_class=DeepClustering_VAE1D, load_from_json = load_from_json)\n",
    "            else:\n",
    "                # load the best model\n",
    "                RELOAD_MODEL_NAME = f\"{self.full_path_to_store}/{self.model_name}.pth\"\n",
    "                checkpoint = torch.load(RELOAD_MODEL_NAME)\n",
    "                if self.verbosity > 0: print(f'Best model {RELOAD_MODEL_NAME} at epoch: ', checkpoint['epoch'])\n",
    "\n",
    "                self.model = DeepClustering_VAE1D(**self.model_kwargs)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # To device\n",
    "            self.model.to(self.device)\n",
    "\n",
    "            if for_eval:\n",
    "                self.model.eval() \n",
    "        \n",
    "        except Exception as e: \n",
    "            print(f\"Error:\\n{e}\\nModel not loaded\\n\")\n",
    "        \n",
    "            \n",
    "    def init_adam_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr = self.learning_rate,\n",
    "        )\n",
    "        if self.use_lr_scheduler:\n",
    "            self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, \n",
    "                mode='min',\n",
    "                factor   = 0.1, # Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "                patience = 5  , #  Number of epochs with no improvement after which learning rate will be reduced. \n",
    "            )\n",
    "            \n",
    "    def compute_vae_loss(self, x_hat, x, z, β=1, monte_carlo_size:int=256, return_all:bool=False):\n",
    "        return vae_loss_function(x_hat, x, z, β, monte_carlo_size=monte_carlo_size, return_all=return_all)\n",
    "    \n",
    "    def compute_total_loss(self, loss, avg_s_loss, γ, use_sum=False):\n",
    "        return compute_total_loss(loss, avg_s_loss, γ, use_sum=use_sum)     \n",
    "    \n",
    "    def training_step(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        train_rec_loss = 0\n",
    "        train_mmd_loss = 0\n",
    "        train_s_loss   = 0\n",
    "\n",
    "        counter = 0\n",
    "        # beta\n",
    "        try:\n",
    "            β_t = self.beta_t(self.epoch)\n",
    "        except: \n",
    "            β_t = 0.0\n",
    "        # Do not apply reparametrisation if Beta is zero!\n",
    "        if β_t > 0.0:\n",
    "            self.model._is_vae = True\n",
    "        else: \n",
    "            self.model._is_vae = False\n",
    "        # gamma    \n",
    "        try:\n",
    "            γ   = self.gamma_t(self.epoch)\n",
    "        except: \n",
    "            γ   = self.γ_init\n",
    "        \n",
    "        # SET\n",
    "        self.β_t = β_t\n",
    "        self.γ   = γ\n",
    "        \n",
    "        # Iterate over train set\n",
    "        for x, y in tqdm.tqdm(self.dataloader_train):\n",
    "            x = x.to(self.device)\n",
    "            # =================== forward =====================\n",
    "            x_hat, z, mu, logvar, best_cl, best_c, best_score, best_K, _  = self.forward(x)\n",
    "            loss, rec_loss, mmd_loss = self.compute_vae_loss(x_hat, x, z, β=β_t, return_all=True)\n",
    "            avg_s_loss = (1 - best_score)/2\n",
    "            loss = self.compute_total_loss(loss, avg_s_loss, γ, use_sum=self.USE_SUM)\n",
    "            # =================== Losses =====================\n",
    "            train_loss += loss.item()\n",
    "            train_rec_loss += rec_loss.item()\n",
    "            train_mmd_loss += mmd_loss.item()\n",
    "            train_s_loss += avg_s_loss\n",
    "            # =================== backward ====================\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # =================== Free CUDA Memory ====================\n",
    "            free_memory([x_hat, mu, logvar])\n",
    "            counter += 1\n",
    "        # =================== compute LOSS ====================\n",
    "        train_divisor = counter\n",
    "        tr_loss = train_loss / train_divisor\n",
    "        train_rec_loss = train_rec_loss / train_divisor\n",
    "        train_mmd_loss = train_mmd_loss / train_divisor\n",
    "        train_s_loss   = train_s_loss   / train_divisor\n",
    "        # === Append Losses ========\n",
    "        self.training_loss.append(tr_loss) ## Full trainloss\n",
    "        \n",
    "        self.mmd_losses.append(train_mmd_loss) ## KLD train loss\n",
    "        self.rec_losses.append(train_rec_loss) ## Rec train loss\n",
    "        self.s_losses.append(train_s_loss)     ## Cluster loss\n",
    "        \n",
    "        return tr_loss, train_rec_loss, train_mmd_loss, train_s_loss\n",
    "        \n",
    "    def eval_step(self):\n",
    "        # =====================================================\n",
    "        # Validation\n",
    "        counter_val = 0\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_rec_loss = 0\n",
    "            val_mmd_loss = 0\n",
    "            val_s_loss   = 0\n",
    "            for x, y in tqdm.tqdm(self.dataloader_val):\n",
    "                x = x.to(self.device)\n",
    "                # =================== forward =====================\n",
    "                x_hat, z, mu, logvar, best_cl, best_c, best_score, best_K, _  = self.forward(x)\n",
    "                loss, rec_loss, mmd_loss = self.compute_vae_loss(x_hat, x, z, β=self.β_t, return_all=True)\n",
    "                avg_s_loss = (1 - best_score)/2\n",
    "                #loss += γ * avg_s_loss\n",
    "                loss = self.compute_total_loss(loss, avg_s_loss, self.γ,  use_sum=self.USE_SUM)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_rec_loss += rec_loss.item()\n",
    "                val_mmd_loss += mmd_loss.item()\n",
    "                val_s_loss   += avg_s_loss\n",
    "                # =================== Free CUDA memory ====================\n",
    "                free_memory([x_hat, mu, logvar])\n",
    "                counter_val += 1\n",
    "\n",
    "        # =================== log ========================        \n",
    "        val_divisor = counter_val\n",
    "        val_loss /= val_divisor\n",
    "        val_rec_loss /= val_divisor\n",
    "        val_mmd_loss /= val_divisor\n",
    "        val_s_loss  /= val_divisor\n",
    "\n",
    "        self.validation_loss.append(val_loss)\n",
    "        \n",
    "\n",
    "        #save best model\n",
    "        if not self.delay_best_model_store or self.epoch > self.beta_t_ending_epoch:\n",
    "            #save best model\n",
    "            self.save_best_model(val_loss, self.epoch, self.model, self.optimizer, vae_loss_function) \n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def train_model(self):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            # Training loop\n",
    "            for epoch in range(0, self.epochs):\n",
    "                self.epoch = epoch\n",
    "                # =====================================================\n",
    "                # Training\n",
    "                tr_loss, train_rec_loss, train_mmd_loss, train_s_loss = self.training_step()\n",
    "                free_memory([])\n",
    "                # =====================================================\n",
    "                # Validation\n",
    "                val_loss = self.eval_step()\n",
    "                # elapsed time\n",
    "                elapsed_time = time.time() - t0\n",
    "                # =====================================================\n",
    "                # Learning Rate stepper\n",
    "                self.current_lr = self.optimizer.param_groups[0]['lr']#lr_scheduler.get_last_lr()[0]\n",
    "                self.learning_rates.append(self.current_lr)\n",
    "                # update learning rate schedule\n",
    "                self.lr_scheduler.step(val_loss) ### NB: ONLY FOR ReduceLROnPlateau\n",
    "                # =================== log ========================\n",
    "                log_line = f'====> Epoch: {self.epoch}\\tTraining loss: {tr_loss:.6f}\\t Validation set loss: {val_loss:.6f}\\tBeta: {self.β_t:.2f}\\tGamma:  {self.γ:.2f}\\tlr: {self.current_lr:.2e}\\tTime: {datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}\\tTrain Rec_loss: {train_rec_loss:.6f}\\tTrain MMD: {train_mmd_loss:.6f}\\tTrain clust_loss: {train_s_loss:.6f}\\n'\n",
    "                if self.verbosity > 0: print(log_line)\n",
    "                write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=log_line)\n",
    "\n",
    "                # Check patience\n",
    "                if self.PATIENCE > 0 and len(self.validation_loss) - np.array(self.validation_loss).argmin() > self.PATIENCE + self.Delta_patience:\n",
    "                    break_log = f\"\\nPatience treshold = {self.PATIENCE} reached.\\nExiting at epoch {self.epoch}.\\n\"\n",
    "                    if self.verbosity > 0: print(break_log)\n",
    "                    write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=break_log)\n",
    "                    break\n",
    "                    \n",
    "            # Store training history in DF\n",
    "            self.store_df()\n",
    "            \n",
    "            if self.verbosity > 0: print(f\"\\n\\nDone\")\n",
    "        # ==== HANDLE ERRORS =================0\n",
    "        except Exception as e:\n",
    "            err_line = f\"\\n\\Error at epoch {epoch}:\\n{e}\\n\\n\"\n",
    "            print(err_line)\n",
    "            write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=err_line)\n",
    "            \n",
    "    \n",
    "    def test_model(self, load_from_json: bool = False):\n",
    "        # Load model\n",
    "        self.load_model(load_from_json)\n",
    "        # Eval\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            test_loss = 0\n",
    "            test_rec_loss = 0\n",
    "            test_mmd_loss = 0\n",
    "            test_s_loss   = 0\n",
    "            test_counter = 0\n",
    "            for x, y in tqdm.tqdm(self.dataloader_test):\n",
    "                x = x.to(self.device)\n",
    "                # =================== forward =====================\n",
    "                x_hat, z, mu, logvar, best_cl, best_c, best_score, best_K, _  = self.forward(x)\n",
    "                loss, rec_loss, mmd_loss = self.compute_vae_loss(x_hat, x, z, β=self.β_t, return_all=True)\n",
    "                avg_s_loss = (1 - best_score)/2\n",
    "                loss = self.compute_total_loss(loss, avg_s_loss, self.γ,  use_sum=self.USE_SUM)\n",
    "\n",
    "                test_rec_loss += rec_loss.item()\n",
    "                test_mmd_loss += mmd_loss.item()\n",
    "                test_s_loss   += avg_s_loss\n",
    "                test_loss     += loss\n",
    "                test_counter += 1\n",
    "                # =================== Free CUDA memory ====================\n",
    "                free_memory([x_hat, mu, logvar])\n",
    "\n",
    "        # =================== log ========================\n",
    "        test_loss /= test_counter\n",
    "\n",
    "        test_loss_log = f\"\\n\\n\\t\\t====> Test loss: {test_loss:6f}  <====\\n\"\n",
    "        print(test_loss_log)\n",
    "        write_line_to_file(LOG_FILE=self.LOG_FILE, log_line=test_loss_log)\n",
    "    \n",
    "    \n",
    "    def store_df(self):\n",
    "        # store as pandas csv\n",
    "        df_train = pd.DataFrame(\n",
    "            {\n",
    "                \"epochs\"          : [ epoch for epoch in range(len(self.training_loss)) ],\n",
    "                \"training_loss\"   : self.training_loss,\n",
    "                \"validation_loss\" : self.validation_loss,\n",
    "                \"beta\"            : [ self.beta_t(epoch)  for epoch in range(len(self.training_loss)) ],\n",
    "                \"gamma\"           : [ self.gamma_t(epoch) for epoch in range(len(self.training_loss)) ],\n",
    "                'lr'              : self.learning_rates, \n",
    "                \"mmd_train_losses\": self.mmd_losses,\n",
    "                \"rec_train_losses\": self.rec_losses,\n",
    "                \"clust_losses\"    : self.s_losses\n",
    "            }\n",
    "        )\n",
    "        df_train.to_csv(f'{self.full_path_to_store}/{self.model_name}_history.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9982b6-5812-458f-9577-48352b061092",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ff94e5-8469-4ccd-8165-532f4066337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATASET_PATH = '/jupyter/notebooks/Article/AstroData/Synthetic/1D/' \n",
    "\n",
    "#BASE_MODEL_NAME = 'Synth_Trainer_DeepClustering'# AE or VAE are automatically added\n",
    "USE_SNN  = True\n",
    "BETA_MIN = 0.000\n",
    "BETA_MAX = 0.000\n",
    "GAMMA_INIT = 0.002\n",
    "\n",
    "beta_t_starting_epoch  : int = 15 \n",
    "beta_t_ending_epoch    : int = 16\n",
    "\n",
    "BASE_MODEL_NAME  = 'Synth_Trainer'\n",
    "BASE_MODEL_NAME += '_SNN' if USE_SNN else '_DNN'\n",
    "BASE_MODEL_NAME += '_DeepClustering_' if GAMMA_INIT > 0.0 else '_'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = ('cpu')\n",
    "\n",
    "kwargs = {\n",
    "    \"n_layers\" : 3,\n",
    "    \"encoding_space_dim\" : 32,\n",
    "    \"latent_space_dim\" : 3,\n",
    "    \"use_latent_space_activation\" : False,\n",
    "    \"min_n_cluster\" : 2,\n",
    "    \"max_n_cluster\" : 3,\n",
    "    \"DO_NORMALIZE\" : True, \n",
    "    \"use_SNN\" : USE_SNN, \n",
    "    # Batch\n",
    "    \"BATCH_SIZE\" : 512,\n",
    "    \"PATIENCE\" : 25,\n",
    "    \"epochs\"   : 100, \n",
    "    # Model name\n",
    "    \"BASE_PATH_TO_STORE\" : './model_data_synth',\n",
    "    \"base_model_name\"    : BASE_MODEL_NAME , \n",
    "    #\n",
    "    \"BETA_MIN\" : BETA_MIN, \n",
    "    \"BETA_MAX\" : BETA_MAX,\n",
    "    'beta_t_starting_epoch' : beta_t_starting_epoch,\n",
    "    'beta_t_ending_epoch'   : beta_t_ending_epoch,\n",
    "    #\n",
    "    \"γ_init\" : GAMMA_INIT,\n",
    "    \"USE_VARYING_GAMMA\": False,\n",
    "    \"USE_SUM\": True,\n",
    "    #\n",
    "    \"random_centroid_init\" : not True ,\n",
    "    #\n",
    "    'CUSTOM_TRANSFORM': not True,  # smooth\n",
    "    'BASE_DATASET_PATH' : BASE_DATASET_PATH ,\n",
    "    #\n",
    "    'device' : device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b186b191-3b27-4770-8435-51f692b88a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset.\n",
      "Using custom transform: False\n",
      "\n",
      "Train set:\n",
      "Test set:\n",
      "\n",
      "Done.\n",
      "DeepClustering_VAE1D(\n",
      "  (final_activation): Sigmoid()\n",
      "  (IKMeans): DC_IterativeKMeans()\n",
      "  (encoder): VAEencoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): SNN(\n",
      "        (network): Sequential(\n",
      "          (fc0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (selu_0): SELU()\n",
      "          (dropout_0): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc1): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (selu_1): SELU()\n",
      "          (dropout_1): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc_2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (fc_mu): Linear(in_features=32, out_features=3, bias=True)\n",
      "    (fc_var): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): VAEdecoder(\n",
      "    (final_activation): Sigmoid()\n",
      "    (decoder): Sequential(\n",
      "      (Dec_SNN): SNN(\n",
      "        (network): Sequential(\n",
      "          (fc0): Linear(in_features=3, out_features=128, bias=False)\n",
      "          (selu_0): SELU()\n",
      "          (dropout_0): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc1): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (selu_1): SELU()\n",
      "          (dropout_1): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc_2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (Dec_act): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Selected costant beta of value: 0.0\n",
      "Delay best model store: False;\n",
      "Delta patience: 0\n",
      "Selected costant beta of value: 0.002\n",
      "Stored JSON\n",
      "DeepClustering_VAE1D(\n",
      "  (final_activation): Sigmoid()\n",
      "  (IKMeans): DC_IterativeKMeans()\n",
      "  (encoder): VAEencoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): SNN(\n",
      "        (network): Sequential(\n",
      "          (fc0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (selu_0): SELU()\n",
      "          (dropout_0): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc1): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (selu_1): SELU()\n",
      "          (dropout_1): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc_2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (fc_mu): Linear(in_features=32, out_features=3, bias=True)\n",
      "    (fc_var): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): VAEdecoder(\n",
      "    (final_activation): Sigmoid()\n",
      "    (decoder): Sequential(\n",
      "      (Dec_SNN): SNN(\n",
      "        (network): Sequential(\n",
      "          (fc0): Linear(in_features=3, out_features=128, bias=False)\n",
      "          (selu_0): SELU()\n",
      "          (dropout_0): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc1): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (selu_1): SELU()\n",
      "          (dropout_1): AlphaDropout(p=0.1, inplace=False)\n",
      "          (fc_2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (Dec_act): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Loss used:\tAECluster\n",
      "\n",
      "Train Size: 161001\tValidation size: 46001\tBatch size: 512\n",
      "Epochs: 100\tPatience: 25\n",
      "Latent space activation: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DeepClustering_trainer(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c2d22-54da-44cf-9d0c-124238d7e82e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f78d9d-0203-4e34-a8ca-32c3ccf79450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.20it/s]\n",
      "100%|██████████| 90/90 [00:19<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.0005897289905179706\n",
      "\n",
      "Saving best model for epoch: 0\n",
      "\n",
      "====> Epoch: 0\tTraining loss: 0.009235\t Validation set loss: 0.000590\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_15:59:02\tTrain Rec_loss: 0.009028\tTrain MMD: 1.096702\tTrain clust_loss: 0.103395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:13<00:00,  4.25it/s]\n",
      "100%|██████████| 90/90 [00:19<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1\tTraining loss: 0.000639\t Validation set loss: 0.000812\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:00:35\tTrain Rec_loss: 0.000440\tTrain MMD: 1.143513\tTrain clust_loss: 0.099629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:13<00:00,  4.27it/s]\n",
      "100%|██████████| 90/90 [00:19<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2\tTraining loss: 0.000643\t Validation set loss: 0.000748\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:02:08\tTrain Rec_loss: 0.000437\tTrain MMD: 1.149146\tTrain clust_loss: 0.102783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:13<00:00,  4.28it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3\tTraining loss: 0.000630\t Validation set loss: 0.000658\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:03:41\tTrain Rec_loss: 0.000435\tTrain MMD: 1.156473\tTrain clust_loss: 0.097387\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.23it/s]\n",
      " 40%|████      | 36/90 [00:08<00:11,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 69/90 [00:15<00:04,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 72/90 [00:15<00:03,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:19<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.0004881479360240822\n",
      "\n",
      "Saving best model for epoch: 4\n",
      "\n",
      "====> Epoch: 4\tTraining loss: 0.000573\t Validation set loss: 0.000488\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:05:15\tTrain Rec_loss: 0.000419\tTrain MMD: 1.154190\tTrain clust_loss: 0.076611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 139/314 [00:33<00:41,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.22it/s]\n",
      "  3%|▎         | 3/90 [00:00<00:23,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 23/90 [00:05<00:13,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 28/90 [00:06<00:12,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n",
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 36/90 [00:07<00:11,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 2;\tuniq: 1\n",
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 52/90 [00:11<00:07,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 69/90 [00:14<00:04,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 89/90 [00:19<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error;\tK: 3;\tuniq: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:19<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.00047707894846098496\n",
      "\n",
      "Saving best model for epoch: 5\n",
      "\n",
      "====> Epoch: 5\tTraining loss: 0.000410\t Validation set loss: 0.000477\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:06:49\tTrain Rec_loss: 0.000325\tTrain MMD: 1.123139\tTrain clust_loss: 0.042634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.23it/s]\n",
      "100%|██████████| 90/90 [00:21<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.00046301788825076073\n",
      "\n",
      "Saving best model for epoch: 6\n",
      "\n",
      "====> Epoch: 6\tTraining loss: 0.000410\t Validation set loss: 0.000463\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:08:25\tTrain Rec_loss: 0.000289\tTrain MMD: 1.107616\tTrain clust_loss: 0.060561\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.24it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 7\tTraining loss: 0.000527\t Validation set loss: 0.000562\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:09:59\tTrain Rec_loss: 0.000259\tTrain MMD: 1.125538\tTrain clust_loss: 0.134036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:13<00:00,  4.26it/s]\n",
      "100%|██████████| 90/90 [00:19<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 8\tTraining loss: 0.000520\t Validation set loss: 0.000493\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:11:32\tTrain Rec_loss: 0.000202\tTrain MMD: 1.161597\tTrain clust_loss: 0.159265\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:13<00:00,  4.26it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 9\tTraining loss: 0.000498\t Validation set loss: 0.000510\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:13:07\tTrain Rec_loss: 0.000163\tTrain MMD: 1.176325\tTrain clust_loss: 0.167575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.22it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10\tTraining loss: 0.000499\t Validation set loss: 0.000512\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:14:42\tTrain Rec_loss: 0.000146\tTrain MMD: 1.197503\tTrain clust_loss: 0.176292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:14<00:00,  4.21it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 11\tTraining loss: 0.000505\t Validation set loss: 0.000521\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:16:16\tTrain Rec_loss: 0.000138\tTrain MMD: 1.218319\tTrain clust_loss: 0.183446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:15<00:00,  4.18it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 12\tTraining loss: 0.000509\t Validation set loss: 0.000552\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-03\tTime: 2023-11-27_16:17:52\tTrain Rec_loss: 0.000131\tTrain MMD: 1.237073\tTrain clust_loss: 0.189182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [01:15<00:00,  4.18it/s]\n",
      "100%|██████████| 90/90 [00:20<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 13\tTraining loss: 0.000511\t Validation set loss: 0.000542\tBeta: 0.00\tGamma:  0.00\tlr: 1.00e-04\tTime: 2023-11-27_16:19:27\tTrain Rec_loss: 0.000126\tTrain MMD: 1.243731\tTrain clust_loss: 0.192737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 72/314 [00:17<00:56,  4.31it/s]"
     ]
    }
   ],
   "source": [
    "model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7fc72-ef89-4248-a7d5-93205580bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.full_path_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977c822-46d2-47dd-9474-081f2790909d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5dcef-8021-4dcc-afd2-4b4f75afb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=120)\n",
    "fig.suptitle(f\"{model.model_name} Training history\")\n",
    "ax.plot(range(1,len(model.training_loss)+1)  , model.training_loss, color='green', linestyle='-', label='train loss')\n",
    "ax.plot(range(1,len(model.validation_loss)+1), model.validation_loss, color='blue', linestyle='-', label='validation loss')\n",
    "\n",
    "if model.beta_t:\n",
    "    beta_arr = np.array([model.beta_t(epoch) for epoch in range(len(model.training_loss))])\n",
    "    if beta_arr.max() > 0:\n",
    "        ax.plot(range(1,len(model.training_loss)+1)  , model.BETA_MAX * np.array(model.mmd_losses) , color='purple', linestyle=':', label='MMD loss')\n",
    "\n",
    "try:\n",
    "    #ax.plot(range(1,len(training_loss)+1), γ*np.array(s_losses), color='orange', linestyle=':', label='train S-loss loss', alpha=0.6)\n",
    "    # fake for label\n",
    "    ax.plot(range(1,len(model.training_loss)+1), model.γ * np.array(model.s_losses), color='orange', linestyle='-.', label='train S-loss loss', alpha=0.6)\n",
    "    ax.plot(range(1,len(model.training_loss)+1), model.rec_losses, color='gold', linestyle='--', label='train Rec loss', alpha=0.6)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "ax.set_xlabel('Epochs',fontsize=14)\n",
    "ax.set_ylabel('Loss',fontsize=14)\n",
    "#ax.set_ylim( np.array(training_loss).min(), np.array(training_loss).max() )\n",
    "#ax.set_ylim( 0., 0.1125) \n",
    "#plt.ylim(ymin = 33, ymax=40.0)\n",
    "\n",
    "\n",
    "if model.beta_t:\n",
    "    beta_arr = np.array( [model.beta_t(epoch) for epoch in range(len(model.training_loss))] )\n",
    "    if beta_arr[0] != beta_arr[-1]:\n",
    "        # twin object for two different y-axis on the sample plot\n",
    "        ax2=ax.twinx()\n",
    "        # make a plot with different y-axis using second axis object\n",
    "        ax2.plot(range(1,len(model.training_loss)+1), beta_arr, color='red', linestyle='-', label='train loss')\n",
    "        ax2.set_ylabel(\"beta(t)\", color=\"red\",fontsize=14)\n",
    "\n",
    "        #ax2.set_ylim(-0.1, beta_arr.max())\n",
    "\n",
    "\n",
    "elif model.gamma_t:\n",
    "    beta_arr = [model.gamma_t(epoch) for epoch in range(len(model.training_loss))]\n",
    "    if beta_arr[0] != beta_arr[-1]:\n",
    "        # twin object for two different y-axis on the sample plot\n",
    "        ax2=ax.twinx()\n",
    "        # make a plot with different y-axis using second axis object\n",
    "        ax2.plot(range(1,len(model.training_loss)+1), beta_arr, color='red', linestyle='-', label='train loss')\n",
    "        ax2.set_ylabel(\"gamma(t)\", color=\"red\",fontsize=14)\n",
    "\n",
    "        ax2.set_ylim(-0.1, 0.5)\n",
    "else:\n",
    "    # plot gamma\n",
    "    # twin object for two different y-axis on the sample plot\n",
    "    ax2=ax.twinx()\n",
    "    # make a plot with different y-axis using second axis object\n",
    "    ax2.plot(range(1,len(model.training_loss)+1), model.γ*np.array(model.s_losses), color='orange', linestyle=':', label='train S-loss loss', alpha=0.6)\n",
    "    ax2.set_ylabel(\"Cluster Loss\", color=\"orange\",fontsize=14)\n",
    "\n",
    "    #ax2.set_ylim(-0.1, 3.0)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "        \n",
    "fig.savefig(f\"{model.full_path_to_store}/{model.model_name}.png\",  bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1432aa7-dd11-478a-bc9c-aa05ae5a72c7",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a13604-8c5e-4465-8981-88b6bf80d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6a795-f4d8-4b55-833d-4255656e8d0d",
   "metadata": {},
   "source": [
    "## Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f108f1-5d10-499d-b214-1e2991ebfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open_model import load_model_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d424bc4-360e-4579-8778-d627bb443a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "path_to_file = '/jupyter/notebooks/AstroDeepClustering/AstroData/2D/val/ADP:2021-05-17T14:08:08:293.h5'\n",
    "\n",
    "with h5py.File(path_to_file, 'r') as _h5:\n",
    "    # Open\n",
    "    ma_xrf = torch.Tensor( np.array(_h5['img'][()], dtype=float) ).float()\n",
    "    ma_xrf = ma_xrf[:, :, :3000]\n",
    "    _shape = ma_xrf.shape\n",
    "    # Rebin \n",
    "    ma_xrf = ma_xrf.cpu()\n",
    "    ma_xrf = rebin_xrf( ma_xrf.reshape(-1, _shape[-1]), n_bins=1024 )\n",
    "    ma_xrf = ma_xrf.reshape(*_shape[:2], 1024)\n",
    "    # Transform\n",
    "    ma_xrf = model.custom_transform_realized(ma_xrf)\n",
    "    ma_xrf = ma_xrf.cpu()\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b0077-4868-4c20-a224-3b25d2c760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del MAXRFVizDataset\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428424c-08fb-44c7-af65-6c13d7547792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_dataset import MAXRFVizDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17347b9f-2d41-49a2-8bbc-0ad1a12a6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_FROM_DISK = not True # <============ HERE =======\n",
    "\n",
    "\n",
    "if LOAD_MODEL_FROM_DISK:\n",
    "    #LOAD_NAME = 'Synth_Trainer_SNN_VAE_2023-09-08_07:22:33_use_sum'\n",
    "    #LOAD_NAME = 'Synth_Trainer_SNNDeepClustering_VAE_2023-09-07_15:51:26_use_sum'\n",
    "    #BASE_MODEL_NAME = 'Synth_Trainer_DeepClustering'# AE or VAE are automatically added\n",
    "    LOAD_NAME = 'Synth_Trainer_SNN_AE_2023-09-12_18:08:04_use_sum'\n",
    "    USE_SNN  = True\n",
    "    BETA_MIN = 1.0\n",
    "    BETA_MAX = 1.0\n",
    "    GAMMA_INIT = 0.0\n",
    "\n",
    "    BASE_MODEL_NAME  = 'Dummy_'\n",
    "    model_kwargs = {\n",
    "        \"n_layers\" : 3,\n",
    "        \"encoding_space_dim\" : 32,\n",
    "        \"latent_space_dim\" : 3,\n",
    "        \"use_latent_space_activation\" : False,\n",
    "        \"min_n_cluster\" : 2,\n",
    "        \"max_n_cluster\" : 3,\n",
    "        \"use_SNN\" : USE_SNN, \n",
    "        #\n",
    "        \"random_centroid_init\" : True ,\n",
    "    }\n",
    "    \n",
    "    kwargs = {\n",
    "        **model_kwargs, \n",
    "        \"DO_NORMALIZE\" : True, \n",
    "        # Batch\n",
    "        \"BATCH_SIZE\" : 512,\n",
    "        \"PATIENCE\" : 25,\n",
    "        \"epochs\"   : 100, \n",
    "        # Model name\n",
    "        \"BASE_PATH_TO_STORE\" : './model_data_synth',\n",
    "        \"base_model_name\"    : BASE_MODEL_NAME , \n",
    "        #\n",
    "        \"BETA_MIN\" : BETA_MIN, \n",
    "        \"BETA_MAX\" : BETA_MAX,\n",
    "        #\n",
    "        \"γ_init\" : GAMMA_INIT,\n",
    "        \"USE_VARYING_GAMMA\": False,\n",
    "        \"USE_SUM\": True,\n",
    "        #\n",
    "        'CUSTOM_TRANSFORM': True,  # smooth\n",
    "    }\n",
    "\n",
    "    model = DeepClustering_trainer(**kwargs)\n",
    "    model.model_name = LOAD_NAME\n",
    "    model.full_path_to_store =  f'./model_data_synth/{LOAD_NAME}'\n",
    "    \n",
    "    model.load_model(load_from_json=not True)\n",
    "    \n",
    "    model.β_t = BETA_MAX\n",
    "    model.gamma_t = GAMMA_INIT\n",
    "    model.γ = GAMMA_INIT\n",
    "else:\n",
    "    model.load_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abd449-db84-4d5d-aab8-af10ac0153d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8ab5f-e6d1-45b9-befe-f223b8075ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_dataset import MAXRFVizDataset\n",
    "path_to_file = '/jupyter/notebooks/AstroDeepClustering/AstroData/Synthetic/AstroDataCube_Generated.h5'\n",
    "#path_to_file = '/jupyter/notebooks/AstroDeepClustering/AstroData/Synthetic/AstroDataCube_Generated_noisy.h5'\n",
    "\n",
    "ma_xrf_dataset = MAXRFVizDataset(\n",
    "    path_to_datacube = path_to_file,\n",
    "    data_name = 'img',\n",
    "    transform = model.custom_transform_realized,\n",
    "    MAX_BIN = 3000, \n",
    "    REBIN_SIZE = 1024,\n",
    ")\n",
    "\n",
    "ma_xrf_dataloader =  DataLoader(\n",
    "    ma_xrf_dataset,  \n",
    "    batch_size=model.BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccaa8c-b3a0-4eeb-b454-746ef3a167d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_xrf = torch.zeros(ma_xrf_dataset.final_shape)\n",
    "transformed_ma_xrf  = torch.zeros_like(ma_xrf)\n",
    "\n",
    "mu = torch.zeros([ma_xrf_dataset.__len__(), model.latent_space_dim])\n",
    "best_cl = torch.zeros(ma_xrf_dataset.__len__())\n",
    "best_c  = []\n",
    "best_score = 0.0\n",
    "best_K = 0.0 \n",
    "s_scores = []\n",
    "\n",
    "# Datacube shape\n",
    "_final_shape = [*ma_xrf_dataset.shape[:2], ma_xrf_dataset.REBIN_SIZE]\n",
    "\n",
    "counter = 0\n",
    "for x in ma_xrf_dataloader:\n",
    "    x = x.nan_to_num(nan=0.0)\n",
    "    # Create Original Datacube\n",
    "    ma_xrf[counter*ma_xrf_dataloader.batch_size:(counter+1)*ma_xrf_dataloader.batch_size, :] = x\n",
    "    # === Forward ======\n",
    "    x = x.to(model.device)\n",
    "    # Forward\n",
    "    _transformed_ma_xrf, _, _mu, _, _best_cl, _best_c, _best_score, _best_K, _s_scores = model.model(x)\n",
    "    # Detach\n",
    "    _transformed_ma_xrf = _transformed_ma_xrf.detach().cpu()\n",
    "    transformed_ma_xrf[counter*ma_xrf_dataloader.batch_size:(counter+1)*ma_xrf_dataloader.batch_size, :] = _transformed_ma_xrf\n",
    "    \n",
    "    _mu = _mu.detach().cpu()\n",
    "    mu[counter*ma_xrf_dataloader.batch_size:(counter+1)*ma_xrf_dataloader.batch_size, :] = _mu\n",
    "    \n",
    "    _best_cl = _best_cl.detach().cpu()\n",
    "    best_cl[counter*ma_xrf_dataloader.batch_size:(counter+1)*ma_xrf_dataloader.batch_size] = _best_cl\n",
    "    \n",
    "    _best_c  = _best_c.detach().cpu()\n",
    "    best_c.append( _best_c  )\n",
    "    \n",
    "    _best_score = _best_score\n",
    "    best_score += best_score\n",
    "    \n",
    "    _best_K   = _best_K\n",
    "    best_K += _best_K \n",
    "    \n",
    "    _s_scores = _s_scores\n",
    "    s_scores.append( _s_scores )\n",
    "    \n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f755c16-8bc3-42e9-9d36-8b1e55875563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loaded_model = model.model.cpu()\n",
    "_final_shape = ma_xrf.shape\n",
    "transformed_ma_xrf, _, mu, _, best_cl, best_c, best_score, best_K, s_scores  = loaded_model( ma_xrf.reshape(-1, _final_shape[-1] ) )\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, dpi=160)\n",
    "ax[0].imshow(ma_xrf.reshape(*_final_shape).sum(dim=-1).detach().cpu().numpy() )\n",
    "ax[0].set_title('Original')\n",
    "ax[1].imshow(transformed_ma_xrf.reshape(*_final_shape).sum(dim=-1).detach().cpu().numpy()  )\n",
    "ax[1].set_title('Decoded')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08882576-dcf6-4923-8764-04286e22df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, dpi=160, figsize = (16,8))\n",
    "ax[0].imshow(ma_xrf.reshape(*_final_shape).sum(dim=-1).detach().cpu().numpy() )\n",
    "ax[0].set_title('Original')\n",
    "ax[1].imshow(transformed_ma_xrf.reshape(*_final_shape).sum(dim=-1).detach().cpu().numpy()  )\n",
    "ax[1].set_title('Decoded')\n",
    "ax[2].imshow( mu.reshape([*_final_shape[:2], mu.shape[-1]] ).sum(dim=-1).detach().cpu().numpy()  )\n",
    "ax[2].set_title('Embedded')\n",
    "plt.savefig(f'./{model.full_path_to_store}/{model.model_name}_integrated_xrf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668cb4e-3c17-4d3e-af95-b9a6a692a530",
   "metadata": {},
   "source": [
    "### Perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2f048-1e90-4249-80d2-14eb7848a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.explore_latent_space import plot_clustered\n",
    "except: \n",
    "    %pip install plotly\n",
    "    from utils.explore_latent_space import plot_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef1272-797f-4acc-9d6a-a8feac6e0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reperform clustering\n",
    "best_cl, best_c, best_score, best_K, s_scores = IterativeKMeans(\n",
    "    mu.cpu(), \n",
    "    min_n_cluster = 3,\n",
    "    max_n_cluster = 5, #model.max_n_cluster, \n",
    "    Niter = 15, random_centroid_init = model.random_centroid_init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f180913-0fa3-4d95-b800-8f7e0452b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1, fig_2, = plot_clustered(\n",
    "    ma_xrf.cpu(),  transformed_ma_xrf.cpu(), mu.cpu(),\n",
    "    best_cl.cpu(), best_c.cpu(), best_score, best_K, s_scores.cpu(),\n",
    "    _final_shape,\n",
    "    MIN_CLUSTER = model.min_n_cluster, MAX_CLUSTER = 5,\n",
    "    # add\n",
    "    tech_name = 'MLM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f56eb-48be-4224-a6ad-e86bad2d37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1.savefig(f'./{model.full_path_to_store}/{model.model_name}_clustered_xrf.pdf')\n",
    "fig_2.savefig(f'./{model.full_path_to_store}/{model.model_name}_iterative_clustering_silhouette_scores.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d5b29-ddf2-4bba-958a-8c488218b9b1",
   "metadata": {},
   "source": [
    "### Visualize Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3c5f3-602f-4904-89ee-e5fe23c349c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.explore_latent_space import plot_latent_space\n",
    "fig_3 = plot_latent_space(\n",
    "    ma_xrf.cpu().detach(), ma_xrf.cpu().detach(), mu.cpu().detach(),\n",
    "    best_cl.cpu().detach(), best_c.cpu().detach(), best_score, best_K, s_scores.cpu().detach(),\n",
    "    _final_shape,\n",
    "    MIN_CLUSTER = model.min_n_cluster, MAX_CLUSTER = 5,\n",
    "    # add\n",
    "    tech_name = 'MLM',\n",
    "    N_CONTOUR_LEVELS = 5,\n",
    "    PLOT_LATENT_SPACE_MA_XRF = True,\n",
    "    PLOT_HIST_LOG_Y_SCALE = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace9a0e-120d-4624-9e31-a911c6638c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3.savefig(f'./{model.full_path_to_store}/{model.model_name}_clustered_latent_space_with_XRF.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e567f-8f73-4caf-a15f-c3d4fdab10ef",
   "metadata": {},
   "source": [
    "### Explore Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263095bb-dba9-488b-83f4-5b08ce44a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.explore_trained_model import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69048b28-a523-402b-8d9e-4f0de3c422de",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = DeepClustering_VAE1D(**model.model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ba969-334e-4f09-96bb-62b61ffee446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_model(\n",
    "    model_to_plot   = model.model.encoder ,\n",
    "    untrained_model = untrained_model.encoder , \n",
    "    # \n",
    "    _suptitle = f\"Encoder weights\",\n",
    "    MAGNIFYING_PERCENTAGE = 120, \n",
    ")\n",
    "\n",
    "try:\n",
    "    fig.savefig(f'./{model.full_path_to_store}/{model.model_name}_Encoder_view.pdf')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6bd60-3523-4c90-a415-1d6d77a67c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_model(\n",
    "    model_to_plot   = model.model.decoder ,\n",
    "    untrained_model = untrained_model.decoder , \n",
    "    # \n",
    "    _suptitle = f\"Decoder weights\",\n",
    "    MAGNIFYING_PERCENTAGE = 120, \n",
    ")\n",
    "\n",
    "try:\n",
    "    fig.savefig(f'./{model.full_path_to_store}/{model.model_name}_Decoder_view.pdf')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9571f44-efe7-41c2-8866-20b7a0875c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558734a-e8e9-4751-995f-0d1de71966cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e813b6e-ecd6-419f-be82-4d7a034c67ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
