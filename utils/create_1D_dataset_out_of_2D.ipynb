{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bc39c7-2341-448f-9229-7448d1a79018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#   MAGIC TRICK FOR HAVING tab, shift+tab COMMANDS!\n",
    "#################################################################################\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da854c78-ccd6-4b12-8ff3-158ce657cebc",
   "metadata": {},
   "source": [
    "# Create 1D Dataset \n",
    "\n",
    "We create a Training/Validation/Test 1D dataset out of the 2D images; To do so, we extract a random percentage of pixels out of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b49712-e95a-4b5a-b6f5-01e2b78c2e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch v.\t1.10.1+cu102\n",
      "TorchVision v.\t0.11.2+cu102\n",
      "\n",
      "number of devices:  1\n",
      "Tesla T4\n",
      "Computation device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import gc\n",
    "from typing import Union\n",
    "import tqdm\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"PyTorch v.\\t{torch.__version__}\")\n",
    "print(f\"TorchVision v.\\t{torchvision.__version__}\\n\")\n",
    "\n",
    "# in torch/pytorch data and models need to be moved in the specific processing unit\n",
    "# this code snippet allows to set the variable \"device\" according to available resoirce (cpu or cuda gpu)\n",
    "if torch.cuda.is_available():\n",
    "    print('number of devices: ', torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\\n\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pretreatment import rebin_xrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662b9b30-26f1-4957-b366-45b7e1c43cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_indeces(_x: Union[np.array, torch.Tensor], _N_shown: int) -> list:\n",
    "    _random_indeces = np.random.choice(_x.shape[0], _N_shown, replace=False) \n",
    "    return (torch.tensor(_random_indeces), _x[_random_indeces,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66a7f7-175b-4b92-9c58-9430224a731e",
   "metadata": {},
   "source": [
    "## CH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a69dff5-fe84-43ce-b01b-17adc47d1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1637.h5: 100%|██████████| 11/11 [07:00<00:00, 38.23s/it]\n",
      "1640.h5: 100%|██████████| 5/5 [02:29<00:00, 29.92s/it]\n",
      ".ipynb_checkpoints: 100%|██████████| 3/3 [01:17<00:00, 25.79s/it]\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_XRF_DATA   = '/jupyter/notebooks/Data/Synthetic_XRF/XRF/'\n",
    "PATH_TO_STORE_DATA = '/jupyter/notebooks/Article/CHData/Synth/1D/'\n",
    "\n",
    "TRAIN_DIR_NAME = 'train'\n",
    "VAL_DIR_NAME   = 'val'\n",
    "TEST_DIR_NAME  = 'test'\n",
    "\n",
    "compression_factor = 9\n",
    "\n",
    "dataset_name = 'img'\n",
    "_N_shown = 20*512*512 // 100\n",
    "\n",
    "dir_names = [TRAIN_DIR_NAME, VAL_DIR_NAME, TEST_DIR_NAME]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    full_path = os.path.join(PATH_TO_XRF_DATA, dir_name)\n",
    "    if not os.path.isdir( full_path ):\n",
    "        raise Exception(f\"{full_path} does not exists.\")\n",
    "    \n",
    "    for f in tqdm.tqdm(os.listdir(full_path), desc=f\"{f}\"):\n",
    "        item_path =  os.path.join(full_path, f) \n",
    "        filename = f.split('.')[0]\n",
    "        if f.endswith('.h5'):\n",
    "            # Open 2D XRF\n",
    "            with h5py.File(item_path, 'r') as _h5:\n",
    "                #_xrf = torch.Tensor( _h5[dataset_name][()] ).float()\n",
    "                _xrf = np.array( _h5[dataset_name][()] , dtype=int)\n",
    "            # Create 1D XRF \n",
    "            _xrf = torch.tensor(_xrf).float()\n",
    "            _xrf = _xrf.reshape(-1, _xrf.shape[-1])\n",
    "            _random_indeces, _x = extract_random_indeces(_xrf, _xrf.shape[0]*20//100)\n",
    "            # Store \n",
    "            if not os.path.isdir(PATH_TO_STORE_DATA):\n",
    "                os.mkdir(PATH_TO_STORE_DATA)\n",
    "            new_item_path = os.path.join(PATH_TO_STORE_DATA, f'{dir_name}.h5')\n",
    "            with h5py.File(new_item_path, 'a') as new_h5: \n",
    "                new_h5.create_dataset(f\"{filename}/pixel\",  data=_random_indeces, compression=\"gzip\", compression_opts=compression_factor)\n",
    "                new_h5.create_dataset(f\"{filename}/hist\" ,  data=_x, compression=\"gzip\", compression_opts=compression_factor)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68817cc9-2300-464e-a1c9-2603cbd615bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AstroDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be619471-67a1-457c-93b7-545844d26d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:49<00:00, 16.37s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.77s/it]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.19s/it]\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_XRF_DATA   = '/jupyter/notebooks/AstroDeepClustering/AstroData/2D/'\n",
    "PATH_TO_STORE_DATA = '/jupyter/notebooks/AstroDeepClustering/AstroData/1D/'\n",
    "\n",
    "TRAIN_DIR_NAME = 'train'\n",
    "VAL_DIR_NAME   = 'val'\n",
    "TEST_DIR_NAME  = 'test'\n",
    "\n",
    "compression_factor = 9\n",
    "\n",
    "MAX_BIN = 3000\n",
    "REBIN_SIZE = 1024\n",
    "SIGNAL_TO_NOISE_THRESHOLD = 25.0\n",
    "STD_FACTOR = + 1.5\n",
    "\n",
    "dataset_name = 'img'\n",
    "_signal_percentage = 100\n",
    "_noise_percentage = 5\n",
    "_zeros_percentage = 5\n",
    "\n",
    "dir_names = [TRAIN_DIR_NAME, VAL_DIR_NAME, TEST_DIR_NAME]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    full_path = os.path.join(PATH_TO_XRF_DATA, dir_name)\n",
    "    if not os.path.isdir( full_path ):\n",
    "        raise Exception(f\"{full_path} does not exists.\")\n",
    "    \n",
    "    for f in tqdm.tqdm(os.listdir(full_path)):\n",
    "        item_path =  os.path.join(full_path, f) \n",
    "        filename = f.split('.')[0]\n",
    "        if f.endswith('.h5'):\n",
    "            # Open 2D XRF\n",
    "            with h5py.File(item_path, 'r') as _h5:\n",
    "                _ex_datacube = np.array( _h5['img'][()] , dtype=np.float32)\n",
    "                _ex_datacube = torch.tensor(_ex_datacube).float()\n",
    "                _ex_datacube[_ex_datacube < 0] = 0.0 # replace everything unphysically under zero\n",
    "                #print( _ex_datacube.shape )\n",
    "                # redice datacube\n",
    "                _red_datacube = _ex_datacube[:, :, :MAX_BIN]\n",
    "                _red_datacube = _red_datacube.reshape(-1, MAX_BIN)\n",
    "                #print( _red_datacube.shape )\n",
    "                # Rebin datacube\n",
    "                _red_datacube = rebin_xrf(_red_datacube, n_bins=REBIN_SIZE)\n",
    "                # Energy smoothing\n",
    "                # TBD\n",
    "                #print( _red_datacube.shape )\n",
    "                _mean_max = max( torch.log(1 + _red_datacube).sum(dim=-1).mean() + STD_FACTOR*torch.log(1 + _red_datacube).sum(dim=-1).std(), SIGNAL_TO_NOISE_THRESHOLD)\n",
    "                #print(f\"_mean_max: {_mean_max}\")\n",
    "                _signal_datacube = _red_datacube[torch.log(1 + _red_datacube).sum(dim=-1) >= _mean_max]\n",
    "                _noise_datacube  = _red_datacube[torch.log(1 + _red_datacube).sum(dim=-1) <  _mean_max]\n",
    "                _zeros_datacube  = _noise_datacube[_noise_datacube.sum(dim=-1) == 0]\n",
    "                # remove all zeros\n",
    "                _noise_datacube = _noise_datacube[_noise_datacube.sum(dim=-1) > 0]\n",
    "                #print(f\"\"\"Signal counts:\\t{_signal_datacube.shape[0]}\\nNoise  counts:\\t{_noise_datacube.shape[0]}\\nSignal/Noise ration:\\t{_signal_datacube.shape[0]/_noise_datacube.shape[0]*100:.4f}%\\n\"\"\")\n",
    "            # Create 1D Dataset \n",
    "            _N_signal = _signal_percentage * _signal_datacube.shape[0] // 100\n",
    "            _signal_indeces, _x_signal = extract_random_indeces(_signal_datacube, _N_signal)\n",
    "            # Add noise\n",
    "            _N_noise = _noise_percentage * _noise_datacube.shape[0] // 100\n",
    "            _noise_indeces, _x_noise = extract_random_indeces(_noise_datacube, _N_noise)\n",
    "            # Add zeros\n",
    "            _N_zeros = _zeros_percentage * _zeros_datacube.shape[0] // 100\n",
    "            _zeros_indeces, _x_zeros = extract_random_indeces(_zeros_datacube, _N_zeros)\n",
    "            # merge\n",
    "            _random_indeces = torch.cat([_signal_indeces, _noise_indeces, _zeros_indeces], dim=-1)\n",
    "            _x = torch.cat([_x_signal, _x_noise, _x_zeros], dim=0)\n",
    "            # Store \n",
    "            if not os.path.isdir(PATH_TO_STORE_DATA):\n",
    "                os.mkdir(PATH_TO_STORE_DATA)\n",
    "            new_item_path = os.path.join(PATH_TO_STORE_DATA, f'{dir_name}.h5')\n",
    "            with h5py.File(new_item_path, 'a') as new_h5: \n",
    "                new_h5.create_dataset(f\"{filename}/pixel\",  data=_random_indeces, compression=\"gzip\", compression_opts=compression_factor)\n",
    "                new_h5.create_dataset(f\"{filename}/hist\",   data=_x,              compression=\"gzip\", compression_opts=compression_factor)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2560ae2-c71e-4584-b048-d2e9e684ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8665])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_random_indeces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad30942-a0f0-4339-9d4d-b751fdb8142b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
